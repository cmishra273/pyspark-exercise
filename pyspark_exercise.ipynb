{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark homework assignment\n",
        "\n",
        "## Context\n",
        "\n",
        "The goal of this assignment is to get view on your coding workflow & style.  Your main focus should be creating performant & robust code for data manipulations.  \n",
        "\n",
        "For a homework assignment, we cannot grant you access to our infrastructure (Cloudera data platform on prem: a spark cluster deployment on Yarn).  Since the focus is on development, we provided a template notebook to get up and running very quickly on Google Colab.  \n",
        "\n",
        "You have the freedom to perform this assignment on any spark3+ infrastructure.  If want to use a local or cloud setup, go for it!\n",
        "\n",
        "Some of the tasks are open for interpretation.  This allows us to assess business understanding and relevant field experience.  These tasks are not pass or fail checks.  During the interview we'll ask details about the choice(s) you made.\n",
        "\n",
        "For the assignment, you'll be working with store location data.  You might be familiar with the phrase \"Location, location, location\" from the real-estate context.  The same house can have a different selling price based on the location.  In fast moving consumer goods (FMCG), location is one of the most crucial aspects:\n",
        "\n",
        "* Proximity & accessibility to customers increases convenience\n",
        "* Proximity to competitors increases market pressure\n",
        "* It has impact on the supply chain\n",
        "\n",
        "## Evaluation criteria\n",
        "\n",
        "1. Software engineering\n",
        "   1. Clean code (e.g. using meaningful names)\n",
        "   1. Robust & efficient code\n",
        "   1. Styling (e.g. PEP8, or Google style guide)\n",
        "   1. Documentation(e.g. docstrings)\n",
        "   1. Design (e.g. SOLID principles)\n",
        "1. Workflow\n",
        "   1. How you use Git\n",
        "   1. How you structure your assignment\n",
        "   1. Owning mistakes\n",
        "   1. Rationale for design decisions\n",
        "   1. Making your solution accessible to others\n",
        "1. Business context\n",
        "   1. GDPR\n",
        "   1. Fast moving consumer goods\n",
        "1.(optional: own infra) System engineering\n",
        "   1. What setup did you use?\n",
        "   1. How did you set it up?\n",
        "\n",
        "## Deliverables we expect\n",
        "\n",
        "1. Private GitHub repo\n",
        "   1. Colab allows you to save to GitHub\n",
        "   1. Invite my username to your private repo as contributor\n",
        "1. README.md with relevant content\n",
        "1. Code relevant to the assignment\n"
      ],
      "metadata": {
        "id": "MMCkWQR0NQh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google colab spark setup"
      ],
      "metadata": {
        "id": "ZWyMNpBNON-p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RiQgV1aNOJD"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.1-bin-hadoop3.tgz\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import environ\n",
        "import findspark"
      ],
      "metadata": {
        "id": "V6xZShWarSWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting environment variables\n",
        "environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "environ[\"SPARK_HOME\"] = \"/content/spark-3.4.1-bin-hadoop3\""
      ],
      "metadata": {
        "id": "Cvm3vu7cNSzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init spark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "8y6JdEBrO43D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# spark.sql.repl.eagerEval.enabled: Property used to format output tables better\n",
        "\n",
        "spark = (\n",
        "    SparkSession\n",
        "    .builder\n",
        "    .appName(\"cg-pyspark-assignment\")\n",
        "    .master(\"local\")\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
        "    .getOrCreate()\n",
        "  )\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "o5-SFL47PQfJ",
        "outputId": "3d5c5d01-f921-4159-88a0-ea449099a5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b22f59183d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://905cac4cec30:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>cg-pyspark-assignment</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the assignment data\n",
        "\n",
        "This will call the api and save the results in current working directory as .json files"
      ],
      "metadata": {
        "id": "QWtk5p_punuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/clp-places > clp-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/okay-places > okay-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/spar-places > spar-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/dats-places > dats-places.json\n",
        "!curl https://ecgplacesmw.colruytgroup.com/ecgplacesmw/v3/nl/places/filter/cogo-colpnts > cogo-places.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-UEk918o3Xt",
        "outputId": "9c1c6320-b449-451b-d1cc-1abcbec23b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  216k    0  216k    0     0   174k      0 --:--:--  0:00:01 --:--:--  174k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  143k    0  143k    0     0   147k      0 --:--:-- --:--:-- --:--:--  147k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  167k    0  167k    0     0   171k      0 --:--:-- --:--:-- --:--:--  171k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 89162    0 89162    0     0   120k      0 --:--:-- --:--:-- --:--:--  120k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  245k    0  245k    0     0   237k      0 --:--:--  0:00:01 --:--:--  237k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hjs0x5zUjL7T",
        "outputId": "16d5cb0f-90e1-4675-e0ee-362b27f555a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment instructions\n",
        "\n",
        "1. Download the data from api\n",
        "1. Create a logger object that logs to a file \"assignment.log\"\n",
        "   1. You can add whatever logging config you want or need\n",
        "   1. At least on Filehandler based on instructions\n",
        "1. implement get_data_by_brand function\n",
        "   1. Follow instructions in docstring\n",
        "   1. df_clp code line should work\n",
        "1. No more handholding ... :-)\n",
        "1. Create a single object (dataframe) that:\n",
        "   1. Contains data from **all brands**\n",
        "      1. Not every brand has the same columns!\n",
        "   1. Drop placeSearchOpeningHours\n",
        "   1. You can keep sellingPartners as an array\n",
        "   1. Extract \"postal_code\" from address\n",
        "   1. Create new column \"province\" derived from postal_code\n",
        "   1. Transform geoCoordinates into lat and lon column\n",
        "   1. One-hot-encode the handoverServices\n",
        "   1. Pretend houseNumber and streetName are GDPR sensitive.\n",
        "      1. How would you anonymize this data for unauthorized users?\n",
        "      1. (optional) Implement the above\n",
        "      1. How would you show the real data to authorized users?\n",
        "      1. (optional) Implement the above\n",
        "1. Save the end result as a parquet file\n",
        "   1. (optional)partitioning?\n",
        "\n",
        "**postal_code** logic:\n",
        "* \"Brussel\": 1000-1299  \n",
        "* \"Waals-Brabant\": 1300-1499  \n",
        "* \"Vlaams-Brabant\": 1500-1999, 3000-3499  \n",
        "* \"Antwerpen\": 2000-2999  \n",
        "* \"Limburg\": 3500-3999  \n",
        "* \"Luik\": 4000-4999  \n",
        "* \"Namen\": 5000-5999  \n",
        "* \"Henegouwen\": 6000-6599,7000-7999  \n",
        "* \"Luxemburg\": 6600-6999  \n",
        "* \"West-Vlaanderen\": 8000-8999  \n",
        "* \"Oost-Vlaanderen\": 9000-9999"
      ],
      "metadata": {
        "id": "VyvIVnyivCpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import statements should go here\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, lit, regexp_extract\n",
        "from pyspark.sql.functions import array_contains, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, ArrayType, LongType, DoubleType, StructType\n",
        "import logging"
      ],
      "metadata": {
        "id": "DlsAnfxdxsT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define schema\n",
        "schema = StructType([\n",
        "    StructField(\"address\", StringType(), True),\n",
        "    StructField(\"branchId\", StringType(), True),\n",
        "    StructField(\"commercialName\", StringType(), True),\n",
        "    StructField(\"ensign\", StringType(), True),\n",
        "    StructField(\"geoCoordinates\", StructType([\n",
        "        StructField(\"latitude\", DoubleType(), True),\n",
        "        StructField(\"longitude\", DoubleType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"handoverServices\", ArrayType(StringType()), True),\n",
        "    StructField(\"isActive\", StringType(), True),\n",
        "    StructField(\"moreInfoUrl\", StringType(), True),\n",
        "    StructField(\"placeId\", LongType(), True),\n",
        "    StructField(\"placeSearchOpeningHours\", ArrayType(StructType([\n",
        "        StructField(\"from\", StringType(), True),\n",
        "        StructField(\"till\", StringType(), True)\n",
        "    ])), True),\n",
        "    StructField(\"placeType\", StringType(), True),\n",
        "    StructField(\"routeUrl\", StringType(), True),\n",
        "    StructField(\"sellingPartners\", ArrayType(StringType()), True),\n",
        "    StructField(\"sourceStatus\", StringType(), True),\n",
        "    StructField(\"temporaryClosures\", ArrayType(StringType()), True),\n",
        "])\n"
      ],
      "metadata": {
        "id": "J0o5_3ZYlMXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(filename='assignment.log', level=logging.INFO,\n",
        "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
        "LOGGER = getLogger()"
      ],
      "metadata": {
        "id": "ShwdnfjSxm2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_handover_services(df):\n",
        "    if 'handoverServices' in df.columns:\n",
        "        df = df.withColumn(\"handoverServices\", col(\"handoverServices\").cast(ArrayType(StringType())))\n",
        "    else:\n",
        "        df = df.withColumn(\"handoverServices\", lit(None).cast(ArrayType(StringType())))\n",
        "    return df\n",
        "\n",
        "def get_data_by_brand(brand: str, logger=logging.getLogger()):\n",
        "    \"\"\"Fetch input data based on brand.\"\"\"\n",
        "    file_path = f'{brand}-places.json'\n",
        "    try:\n",
        "        df = spark.read.schema(schema).json(file_path)\n",
        "        if df.count() == 0:\n",
        "            logger.warning(f\"No data found for brand: {brand}\")\n",
        "        df = df.withColumn('brand', lit(brand))\n",
        "        df = normalize_handover_services(df)\n",
        "        logger.info(f\"Data loaded successfully for brand: {brand}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error loading data for brand: {brand}, Error: {str(e)}\")\n",
        "        raise e"
      ],
      "metadata": {
        "id": "_wZhHFGFXkW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data for all brands\n",
        "brands = [\"clp\", \"okay\", \"spar\", \"dats\", \"cogo\"]\n",
        "dfs = [get_data_by_brand(brand) for brand in brands]"
      ],
      "metadata": {
        "id": "chz34qgi0vvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine dataframes\n",
        "combined_df = dfs[0]\n",
        "for df in dfs[1:]:\n",
        "    combined_df = combined_df.unionByName(df, allowMissingColumns=True)\n",
        "\n",
        "# Drop placeSearchOpeningHours\n",
        "combined_df = combined_df.drop(\"placeSearchOpeningHours\")\n",
        "\n",
        "# Extract postal_code\n",
        "combined_df = combined_df.withColumn(\"postal_code\", regexp_extract(col(\"address\"), r\"\\d{4}\", 0))\n",
        "\n",
        "# Define postal code to province mapping\n",
        "postal_code_province = [\n",
        "    (1000, 1299, \"Brussel\"),\n",
        "    (1300, 1499, \"Waals-Brabant\"),\n",
        "    (1500, 1999, \"Vlaams-Brabant\"),\n",
        "    (3000, 3499, \"Vlaams-Brabant\"),\n",
        "    (2000, 2999, \"Antwerpen\"),\n",
        "    (3500, 3999, \"Limburg\"),\n",
        "    (4000, 4999, \"Luik\"),\n",
        "    (5000, 5999, \"Namen\"),\n",
        "    (6000, 6599, \"Henegouwen\"),\n",
        "    (7000, 7999, \"Henegouwen\"),\n",
        "    (6600, 6999, \"Luxemburg\"),\n",
        "    (8000, 8999, \"West-Vlaanderen\"),\n",
        "    (9000, 9999, \"Oost-Vlaanderen\")\n",
        "]\n",
        "\n",
        "def get_province(postal_code):\n",
        "    code = int(postal_code)\n",
        "    for start, end, province in postal_code_province:\n",
        "        if start <= code <= end:\n",
        "            return province\n",
        "    return None\n",
        "\n",
        "get_province_udf = udf(get_province, StringType())\n",
        "\n",
        "# Add province column\n",
        "combined_df = combined_df.withColumn(\"province\", get_province_udf(col(\"postal_code\")))\n",
        "\n",
        "# Transform geoCoordinates into lat and lon columns\n",
        "combined_df = combined_df.withColumn(\"latitude\", col(\"geoCoordinates.latitude\"))\n",
        "combined_df = combined_df.withColumn(\"longitude\", col(\"geoCoordinates.longitude\"))\n",
        "\n",
        "# One-hot encode the handoverServices\n",
        "distinct_services = combined_df.selectExpr(\"explode(handoverServices)\").distinct().collect()\n",
        "for service in distinct_services:\n",
        "    service_name = service[0]\n",
        "    combined_df = combined_df.withColumn(service_name, array_contains(col(\"handoverServices\"), service_name))\n",
        "\n",
        "# Save the end result as a parquet file\n",
        "combined_df.write.parquet(\"processed_data.parquet\")"
      ],
      "metadata": {
        "id": "raCPEOOs8tWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "52z6DU2Vi-Am"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
